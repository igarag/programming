{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy\n",
    "\n",
    "Installing typing:\n",
    "\n",
    "```python\n",
    "pip install spacy\n",
    "```\n",
    "\n",
    "The following demo is extracted from spaCy site and [Real Python web site](https://realpython.com/natural-language-processing-spacy-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm # english\n",
    "#!python -m spacy download es_core_news_sm # spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "text = \"Esto es una demo para familiarizarse con la librería spaCy, que se utiliza para el procesamiento del lenguaje natural\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Esto', 'una demo', 'familiarizarse', 'la librería', 'que', 'se', 'el procesamiento', 'lenguaje']\n",
      "Verbs: ['utilizar']\n"
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La frase es: ['Esto', 'es', 'una', 'prueba', 'de', 'texto', 'en', 'español', 'para', 'ver', 'si', 'la', 'librería', 'SpaCy', 'es', 'capaz', 'de', 'sacar', 'los', 'verbos', 'y', 'sustantivos', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "file_name = './data/sample_text.txt'\n",
    "introduction_file_text = open(file_name).read()\n",
    "text_to_doc = nlp(introduction_file_text)\n",
    "\n",
    "print(\"La frase es:\", [token.text for token in text_to_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los sustantivos son:  ['probar', 'texto', 'español', 'librería', 'verbo']\n",
      "Los verbos son:  ['sacar']\n"
     ]
    }
   ],
   "source": [
    "print(\"Los sustantivos son: \", [token.lemma_ for token in text_to_doc if token.pos_ == \"NOUN\"])\n",
    "print(\"Los verbos son: \", [token.lemma_ for token in text_to_doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sencence detection\n",
    "\n",
    "Is the process of locating the start and end of sentences in a given text. You’ll use these units when you’re processing your text to perform tasks such as **part of speech tagging** and **entity extraction**.\n",
    "\n",
    "In spaCy, the sents property is used to extract sentences. Here’s how you would extract the total number of sentences and the sentences for a given input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dado que hemos cargado el diccionario en español en esta segunda parte probaremos la detección de frases con texto en español.\n",
      "Puede ser muy útil para procesar pequeñas porciones de texto de esta manera las conclusiones obtenidas pueden ser mejores.\n",
      "El resultado es el siguiente.\n"
     ]
    }
   ],
   "source": [
    "text_with_sentences = ('Dado que hemos cargado el diccionario en español en esta segunda parte'\n",
    "                       ' probaremos la detección de frases con texto en español.'\n",
    "                       ' Puede ser muy útil para procesar pequeñas porciones de texto'\n",
    "                       ' de esta manera las conclusiones obtenidas pueden ser mejores.'\n",
    "                       ' El resultado es el siguiente.')\n",
    "\n",
    "sentence_to_doc = nlp(text_with_sentences)\n",
    "sentences = list(sentence_to_doc.sents)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is the next step after sentence detection. It allows you to identify the basic units in your text. These basic units are called tokens. Tokenization is useful because it breaks a text into meaningful units. These units are used for further analysis, like part of speech tagging. Note how spaCy preserves the starting index of the tokens. It’s useful for in-place word replacement.\n",
    "\n",
    "In spaCy, you can print tokens by iterating on the Doc object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in sentence_to_doc:\n",
    "    print(token, token.idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy provides various attributes for the Token class:\n",
    "\n",
    "- `text_with_ws` prints token text with trailing space (if present).\n",
    "- `is_alpha` detects if the token consists of alphabetic characters or not.\n",
    "- `is_punct` detects if the token is a punctuation symbol or not.\n",
    "- `is_space` detects if the token is a space or not.\n",
    "- `shape_` prints out the shape of the word.\n",
    "- `is_stop` detects if the token is a stop word or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in sentence_to_doc:\n",
    "    print (token,\n",
    "           token.idx,\n",
    "           token.text_with_ws,\n",
    "           token.is_alpha,\n",
    "           token.is_punct,\n",
    "           token.is_space,\n",
    "           token.shape_,\n",
    "           token.is_stop, sep=\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tokenization\n",
    "\n",
    "- `nlp.vocab` is a storage container for special cases and is used to handle cases like contractions and emoticons.\n",
    "- `prefix_search` is the function that is used to handle preceding punctuation, such as opening parentheses.\n",
    "- `infix_finditer` is the function that is used to handle non-whitespace separators, such as hyphens.\n",
    "- `suffix_search` is the function that is used to handle succeeding punctuation, such as closing parentheses.\n",
    "- `token_match` is an optional boolean function that is used to match strings that should never be split. It overrides the previous rules and is useful for entities like URLs or numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "text = ('Gus Proto is a Python developer currently working for a '\n",
    "        'London-based Fintech company.'\n",
    "        'He is interested in learning Natural Language Processing.')\n",
    "custom_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "prefix_re = spacy.util.compile_prefix_regex(custom_nlp.Defaults.prefixes)\n",
    "suffix_re = spacy.util.compile_suffix_regex(custom_nlp.Defaults.suffixes)\n",
    "\n",
    "infix_re = re.compile(r'''[-~]''')\n",
    "\n",
    "def customize_tokenizer(nlp):\n",
    "    # Adds support to use `-` as the delimiter for tokenization\n",
    "    return Tokenizer(nlp.vocab, \n",
    "                     prefix_search=prefix_re.search,\n",
    "                     suffix_search=suffix_re.search,\n",
    "                     infix_finditer=infix_re.finditer,\n",
    "                     token_match=None\n",
    "                     )\n",
    "\n",
    "custom_nlp.tokenizer = customize_tokenizer(custom_nlp)\n",
    "custom_tokenizer_about_doc = custom_nlp(text)\n",
    "\n",
    "print([token.text for token in custom_tokenizer_about_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "\n",
    "In the English language, some examples of stop words are `the`, `are`, `but`, and `they`. Most sentences need to contain stop words in order to be full sentences that make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "len(spacy_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for stop_word in list(spacy_stopwords)[:10]:\n",
    "    print(stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we remove those words from the text, we are removing \"noise\" that can confuse the objective of the phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = ('Gus Proto is a Python developer currently working for a '\n",
    "        'London-based Fintech company.'\n",
    "        'He is interested in learning Natural Language Processing.')\n",
    "text_to_doc = nlp(text)\n",
    "\n",
    "for token in text_to_doc:\n",
    "    if not token.is_stop:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a **list** of tokens not containing stop words. The output can be used to form a sentence with no stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gus, Proto, is, a, Python, developer, currently, working, for, a, London, -, based, Fintech, company, ., is, interested, in, learning, Natural, Language, Processing, .]\n"
     ]
    }
   ],
   "source": [
    "about_no_stopword_doc = [token for token in text_to_doc if not token.is_stop]\n",
    "print (about_no_stopword_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "text = 'Soy un texto que pide a gritos que lo procesen. Por eso yo canto, tú cantas, ella canta, nosotros cantamos, cantáis, cantan…'\n",
    "doc = nlp(text)\n",
    "lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "\n",
    "print(\"Original\\t\\t  Lemma\")\n",
    "print(\"----------------------------------\")\n",
    "for token in doc:\n",
    "    print(token, \"\\t\\t\", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency\n",
    "\n",
    "You can now convert a given text into tokens and perform statistical analysis over it. This analysis can give you various insights about word patterns, such as common words or unique words in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "complete_text = ('Gus Proto is a Python developer currently'\n",
    "                 'working for a London-based Fintech company. He is'\n",
    "                 ' interested in learning Natural Language Processing.'\n",
    "                 ' There is a developer conference happening on 21 July'\n",
    "                 ' 2019 in London. It is titled \"Applications of Natural'\n",
    "                 ' Language Processing\". There is a helpline number '\n",
    "                 ' available at +1-1234567891. Gus is helping organize it.'\n",
    "                 ' He keeps organizing local Python meetups and several'\n",
    "                 ' internal talks at his workplace. Gus is also presenting'\n",
    "                 ' a talk. The talk will introduce the reader about \"Use'\n",
    "                 ' cases of Natural Language Processing in Fintech\".'\n",
    "                 ' Apart from his work, he is very passionate about music.'\n",
    "                 ' Gus is learning to play the Piano. He has enrolled '\n",
    "                 ' himself in the weekend batch of Great Piano Academy.'\n",
    "                 ' Great Piano Academy is situated in Mayfair or the City'\n",
    "                 ' of London and has world-class piano instructors.')\n",
    "\n",
    "complete_doc = nlp(complete_text)\n",
    "\n",
    "# Remove stop words and punctuation symbols\n",
    "words = [token.text for token in complete_doc\n",
    "         if not token.is_stop and not token.is_punct]\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words = word_freq.most_common(5)\n",
    "print (common_words, \"\\n\")\n",
    "\n",
    "# Unique words\n",
    "unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
    "print (unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following example is the same that above but with **stop words**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('is', 10), ('a', 5), ('in', 5), ('Gus', 4), ('of', 4)]\n"
     ]
    }
   ],
   "source": [
    "words_all = [token.text for token in complete_doc if not token.is_punct]\n",
    "word_freq_all = Counter(words_all)\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words_all = word_freq_all.most_common(5)\n",
    "print (common_words_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four out of five of the most common words are stop words, which don’t tell you much about the text. If you consider stop words while doing word frequency analysis, then you won’t be able to derive meaningful insights from the input text. This is why removing stop words is so important.\n",
    "\n",
    "Part of speech or POS is a grammatical role that explains how a particular word is used in a sentence. There are eight parts of speech:\n",
    "\n",
    "- Noun\n",
    "- Pronoun\n",
    "- Adjective\n",
    "- Verb\n",
    "- Adverb\n",
    "- Preposition\n",
    "- Conjunction\n",
    "- Interjection\n",
    "\n",
    "Part of speech tagging is the process of assigning a POS tag to each token depending on its usage in the sentence. POS tags are useful for assigning a syntactic category like noun or verb to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in complete_doc:\n",
    "    print (token, token.tag_, token.pos_, spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "adjectives = []\n",
    "for token in complete_doc:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == 'ADJ':\n",
    "        adjectives.append(token)\n",
    "print(nouns, \"\\n\")\n",
    "print(adjectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Using displaCy\n",
    "\n",
    "spaCy comes with a built-in visualizer called displaCy. You can use it to visualize a dependency parse or named entities in a browser or a Jupyter notebook.\n",
    "\n",
    "You can use displaCy to find POS tags for tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"2f766155b1864ab089aa831aeeea23bd-0\" class=\"displacy\" width=\"1450\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">He</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">interested</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">learning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Natural</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Language</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">Processing.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2f766155b1864ab089aa831aeeea23bd-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2f766155b1864ab089aa831aeeea23bd-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2f766155b1864ab089aa831aeeea23bd-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2f766155b1864ab089aa831aeeea23bd-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M395.0,179.0 L403.0,167.0 387.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2f766155b1864ab089aa831aeeea23bd-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2f766155b1864ab089aa831aeeea23bd-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2f766155b1864ab089aa831aeeea23bd-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2f766155b1864ab089aa831aeeea23bd-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2f766155b1864ab089aa831aeeea23bd-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2f766155b1864ab089aa831aeeea23bd-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2f766155b1864ab089aa831aeeea23bd-0-5\" stroke-width=\"2px\" d=\"M1120,177.0 C1120,89.5 1270.0,89.5 1270.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2f766155b1864ab089aa831aeeea23bd-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,179.0 L1112,167.0 1128,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2f766155b1864ab089aa831aeeea23bd-0-6\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1275.0,2.0 1275.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2f766155b1864ab089aa831aeeea23bd-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,179.0 L1283.0,167.0 1267.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "about_interest_text = ('He is interested in learning'\n",
    "    ' Natural Language Processing.')\n",
    "about_interest_doc = nlp(about_interest_text)\n",
    "displacy.render(about_interest_doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a preprocessing function that takes text as input and applies the following operations:\n",
    "\n",
    "- Lowercases the text\n",
    "- Lemmatizes each token\n",
    "- Removes punctuation symbols\n",
    "- Removes stop words\n",
    "\n",
    "\n",
    "A preprocessing function converts text to an analyzable format. It’s necessary for most NLP tasks. Here’s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gus', 'proto', 'python', 'developer', 'currentlyworking', 'london', 'base', 'fintech', 'company', 'interested', 'learn', 'natural', 'language', 'processing', 'developer', 'conference', 'happen', '21', 'july', '2019', 'london', 'title', 'applications', 'natural', 'language', 'processing', 'helpline', 'number', 'available', '+1', '1234567891', 'gus', 'help', 'organize', 'keep', 'organize', 'local', 'python', 'meetup', 'internal', 'talk', 'workplace', 'gus', 'present', 'talk', 'talk', 'introduce', 'reader', 'use', 'case', 'natural', 'language', 'processing', 'fintech', 'apart', 'work', 'passionate', 'music', 'gus', 'learn', 'play', 'piano', 'enrol', 'weekend', 'batch', 'great', 'piano', 'academy', 'great', 'piano', 'academy', 'situate', 'mayfair', 'city', 'london', 'world', 'class', 'piano', 'instructor']\n"
     ]
    }
   ],
   "source": [
    "def is_token_allowed(token):\n",
    "    '''\n",
    "        Only allow valid tokens which are not stop words\n",
    "        and punctuation symbols.\n",
    "    '''\n",
    "    if (not token or not token.string.strip() or\n",
    "        token.is_stop or token.is_punct):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def preprocess_token(token):\n",
    "    # Reduce token to its lowercase lemma form\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "\n",
    "for token in complete_doc:\n",
    "    if is_token_allowed(token):\n",
    "        preprocess_token(token)\n",
    "\n",
    "print(complete_filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-Based Matching\n",
    "\n",
    "Rule-based matching can use regular expressions to extract entities (such as phone numbers) from an unstructured text. It’s different from extracting text using regular expressions only in the sense that regular expressions don’t consider the lexical and grammatical attributes of the text.\n",
    "\n",
    "With rule-based matching, you can extract a first name and a last name, which are always proper nouns.\n",
    "\n",
    "In this example, pattern is a list of objects that defines the combination of tokens to be matched. Both POS tags in it are PROPN (proper noun). So, the pattern consists of two objects in which the POS tags for both tokens should be PROPN. This pattern is then added to Matcher using FULL_NAME and the the match_id. Finally, matches are obtained with their starting and end indexes.\n",
    "\n",
    " Here, some attributes of the token are also used:\n",
    "\n",
    "- `ORTH` gives the exact text of the token.\n",
    "- `SHAPE` transforms the token string to show orthographic features.\n",
    "- `OP` defines operators. Using ? as a value means that the pattern is optional, meaning it can match 0 or 1 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus Proto\n",
      "Natural Language\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "#matcher = Matcher(nlp.vocab)\n",
    "\n",
    "text = ('Gus Proto is a Python developer currently working for a '\n",
    "        'London-based Fintech company.'\n",
    "        'He is interested in learning Natural Language Processing.')\n",
    "\n",
    "conference_org_text = ('There is a developer conference'\n",
    "    'happening on 21 July 2019 in London. It is titled'\n",
    "    ' \"Applications of Natural Language Processing\".'\n",
    "    ' There is a helpline number available'\n",
    "    ' at (123) 456-789')\n",
    "\n",
    "text_to_doc = nlp(text)\n",
    "conference_org_doc = nlp(conference_org_text)\n",
    "\n",
    "\n",
    "def extract_full_name(nlp_doc):\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    matcher.add('FULL_NAME', None, pattern)\n",
    "    matches = matcher(nlp_doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_doc[start:end]\n",
    "        return span.text\n",
    "\n",
    "def extract_phone_number(nlp_doc):\n",
    "    pattern = [{'ORTH': '('}, {'SHAPE': 'ddd'},\n",
    "               {'ORTH': ')'}, {'SHAPE': 'ddd'},\n",
    "               {'ORTH': '-', 'OP': '?'},\n",
    "               {'SHAPE': 'ddd'}]\n",
    "    matcher.add('PHONE_NUMBER', None, pattern)\n",
    "    matches = matcher(nlp_doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_doc[start:end]\n",
    "        return span.text\n",
    "\n",
    "    \n",
    "print(extract_full_name(text_to_doc))\n",
    "print(extract_phone_number(conference_org_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
